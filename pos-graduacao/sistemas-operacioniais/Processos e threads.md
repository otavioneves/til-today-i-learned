### Seção 1
- Um processo possui o seu próprio espaço de endereçamento, composto de três regiões: uma região de texto, onde é armazenado o código a ser executado pelo processador; uma região de dados, onde são armazenadas as variáveis e a memória, que é alocada dinamicamente durante a execução do processo; e uma região de pilha, onde são armazenadas as instruções e as variáveis locais para as chamadas ativas do procedimento, que fazem a pilha crescer quando são emitidas e diminuem quando o procedimento retorna às chamadas.
- Uma das responsabilidades do sistema operacional é garantir que cada processo tenha acesso à mesma quantidade de tempo de uso de CPU. Entretanto, o número superior de processos, comparado ao número de CPUs disponíveis, somado à concorrência na execução dos processos, faz com que o trabalho do sistema operacional seja mais difícil.
- Um processo usa diferentes estados, como: o estado de execução, que indica que um processo está sendo executado pelo processador; o estado “de pronto”, no qual o processo informa ao sistema operacional que já pode ser executado e está aguardando por um processador que esteja disponível; e o estado bloqueado, quando o processo está aguardando pela finalização de um evento, por exemplo, uma requisição de E/S para prosseguir com sua execução.
- Cabe ao sistema operacional conhecer os processos e seus respectivos estados. Para realizar essa tarefa, ele utiliza duas listas, uma que armazena os processos em estado de pronto, lista de prontos; e outra que armazena os processos em estado bloqueado, lista de bloqueados.
- Os processos da lista de prontos são armazenados de acordo com a sua prioridade, da mais alta para a mais baixa.
- Os sistemas operacionais são responsáveis pelos processos, prestando serviços essenciais a eles, como criação, destruição, suspensão, retomada, mudança de prioridade, bloqueio, ativação e a comunicação interprocessos (Interprocess Communication – IPC).
- A lista de prontos recebe processos de acordo com o que um usuário executa de programas em seu computador.
- Os processos vão galgando posições na lista de prontos, deixando o estado de pronto para o estado de execução, sofrendo uma transição de estado.
- Cabe a uma entidade do sistema operacional, chamada despachante, a responsabilidade de indicar ao processo, que ocupa a primeira posição na lista de prontos, que ele
pode fazer uso de um processador, recebendo essa atividade o nome de despacho.
- O sistema operacional utiliza um timer, isto é, um temporizador de intervalo que permite que um processo seja executado durante um intervalo de tempo específico, chamado quantum 1 . Caso o processo não libere o processador, após o seu limite de tempo de execução expirar, o timer cria uma interrupção, permitindo que o sistema operacional recupere
o controle sobre o processador.
- O processo terá o seu estado alterado de execução para pronto, pelo sistema operacional, que também mudará o estado do primeiro processo da fila de pronto para execução,
iniciando de novo o timer.
- 1) Quando um processo é despachado, o sistema operacional muda seu estado de pronto para execução.
- 2) Quando um evento que é solicitado por um processo que está no estado bloqueado se encerra, ele muda de bloqueado para pronto.
- 3) Quando o quantum de execução de um processo se encerra e ele conseguiu realizar a(s) tarefa(s) desejada(s), ele muda de execução para pronto.
- 4) Quando o quantum de execução de um processo se encerra e ele não conseguiu realizar a(s) tarefa(s) desejada(s), ele muda de execução para bloqueado. Essa é a única transição de estado que é feita pelo próprio processo.
- Quando um processo é criado pelo sistema operacional, ele recebe um número para a sua identificação, o Número de Identificação de Processo (Process Identification Number – PIN).
- Ainda, recebe um conjunto de informações, denominado Bloco de Controle de Processo ou descritor de processo (Process Control Block – PCB), que auxiliará o sistema operacional a gerenciá-lo.
- A tabela de processos permite que o sistema operacional acesse mais rapidamente os PCBs. Quando um processo é encerrado, voluntariamente ou não, o sistema operacional retira o registro desse processo da tabela de processos, disponibilizando os recursos que ele estava utilizando e liberando-os para outros.
- Para detectar possíveis ameaças à segurança, ou para depuração (debug) do software em execução, os sistemas operacionais permitem aos administradores, usuários e processos realizarem a suspensão de um processo. Quando isso ocorre, ele não é destruído, mas retirado da disputa pelo tempo de execução na CPU sem previsão de retorno.
- Quando um processo está em execução e suspende a si mesmo, ele passa para o estado suspenso-pronto, que posteriormente será alterado para pronto. Quando a suspensão é feita por um outro processo, temos duas opções: se o processo suspenso estiver no estado pronto, o seu estado mudará para suspenso-pronto; porém, se ele estiver no estado bloqueado, o seu estado mudará para suspenso-bloqueado.
- Dizemos que um sistema operacional realiza um chaveamento de contexto quando ele interrompe a execução de um processo e começa a executar outro que estava no estado pronto. Quando isso ocorre, o sistema operacional salva o contexto de execução do processo que estava no estado execução no PCB desse processo.
- As interrupções habilitam o software a responder aos sinais do hardware. Para poder gerenciar melhor as interrupções, o sistema operacional usa um conjunto de instruções, chamado tratador de interrupção, no qual ele armazena a resposta para cada tipo de interrupção que pode ocorrer, controlando melhor o processador.
- Os ambientes de multiprogramação e os ambientes de rede obrigaram os sistemas operacionais a criarem formas de comunicação interprocessos (IPC) que garantem a coordenação (sincronização) das atividades dos processos fazendo uso de sinais e trocas de mensagem para eles se comunicarem entre si.
- Sinais são interrupções de software que notificam ao processo que um evento ocorreu. Eles não permitem que os processos troquem dados entre si. Fica sob responsabilidade do sistema operacional determinar qual processo deverá receber o sinal e como ele será respondido pelo processo que o recebeu.
- Ao receber um sinal, os processos podem: capturá-lo, quando o processo especifica uma rotina utilizada pelo sistema operacional para emitir o sinal; ignorá-lo, quando o processo depende de uma ação-padrão realizada pelo sistema operacional para que o sinal recebido possa ser tratado; ou mascará-lo, quando o processo informa ao sistema
operacional que não deseja mais receber sinais daquele tipo até que a máscara do sinal seja bloqueada.
- Com o aumento do interesse pelo uso de sistemas distribuídos, a IPC passou a ser realizada por trocas de mensagens, que podem ser 88 Sistemas Operacionais
unidirecionais, isto é, quando um processo atua como emissor e outro como receptor da mensagem; ou bidirecional, em que cada um dos processos pode atuar como emissor ou receptor. As mensagens podem ter envios bloqueantes (comunicação síncrona) ou envios não bloqueantes (comunicação assíncrona).

### Seção 2
- Um thread permite ao sistema operacional executar uma tarefa de modo independente dos outros processos ou threads, sendo chamado de processo leve (Lightweight Process – LWP). Apesar dessa independência, os threads são criados com base em processo tradicional, chamado de processo pesado (Heavyweight Process – HWP). Threads usam um subconjunto dos recursos utilizados por um processo comum, como: os registradores, a pilha e os Dados Específicos de Threads (Thread-Specific Data – TSD). O espaço de endereço e outras informações globais são compartilhadas pelos threads com o processo pesado.
- O padrão POSIX é uma especificação que busca definir padrões para que os sistemas operacionais sejam compatíveis entre si. Ele define uma API, os shells de linha de comando e as interfaces dos aplicativos para o UNIX e os sistemas com base nele.
- Quando criamos um thread com a linguagem de programação Java, ele está no estado nascido (born), permanecendo assim até que o programa o inicie e passe para o estado pronto
(runnable – executável). Quando ele tem acesso a um processador e começa a ser executado, muda para o estado em execução. Finalmente, indo para o estado morto (dead), que é quando termina sua tarefa ou é encerrado, liberando seus recursos no sistema.
- Apesar de a forma de se implementar threads variar entre os sistemas operacionais, na sua grande maioria, eles utilizam os três modelos mais conhecidos: threads de usuário, threads de núcleo e threads de usuário e de núcleo.
- O sistema operacional visualiza todos os threads que compõem um processo multithread, como apenas um único bloco de execução, que é despachado de uma só vez, e não por thread. Isso recebe o nome de mapeamento de thread muitos-para-um.
- Como desvantagens no seu uso, destacamos: o núcleo (core) do sistema operacional considera um processo multithread como sendo um único bloco de execução, isso impede que o sistema operacional despache threads para serem executados em vários processadores simultaneamente; outro ponto negativo é que, caso um dos threads realize uma requisição para um dispositivo de E/S, todo o processo ficará bloqueado até que a requisição seja encerrada.
- A combinação de threads de usuário e threads de núcleo usa o mapeamento de threads muitos-para-muitos (mapeamento de threads m-to-n), no qual o número de threads de usuário e threads de núcleo não precisa ser igual. Em comparação com o mapeamento de threads um-para-um, o mapeamento de threads muitos-para-muitos consegue reduzir a sobrecarga do sistema operacional, implementando um reservatório de threads (thread pooling), em que a aplicação informa ao sistema operacional o número de threads de núcleo que precisa.

### Seção 3
- Se houver mais de um thread em execução no sistema operacional, dizemos que esses threads são concorrentes entre si. Eles podem estar em execução de maneira independente ou cooperativa. Quando a execução é independente, mas os threads eventualmente se comunicam, chamamos esse evento de execução assíncrona.
 - Enquanto um thread está manipulando a variável, os demais precisam esperar (exclusão mútua). Cabe ao sistema operacional organizar esse processo, limitando o acesso à variável para apenas um dos threads e enfileirando os demais, que aguardam a sua vez. Esse processo é chamado de serialização.
 - Quando um thread acessa uma área que possui dados que podem ser modificados, chamada de seção ou região crítica, ele deve observar se o thread deseja realizar uma operação de leitura que não vai, por exemplo, afetar o conteúdo armazenado. Nesse caso, podemos encontrar threads concorrentes acessando a mesma região crítica.
 - Quatro condições precisam ser atendidas para que tenhamos uma boa solução de gerenciamento ao acesso da região crítica:<br>
a. Dois processos não podem estar acessando simultaneamente as suas regiões críticas.<br>
b. Não há certeza no que se refere à velocidade e ao número de CPUs disponíveis.<br>
c. Um processo que está em execução, fora da sua região crítica, não pode bloquear outros processos.<br>
d. Um processo não pode ficar esperando infinitamente para acessar a região crítica.
- Quando pensamos em uma CPU, mono ou multiprocessada, as estratégias citadas anteriormente são válidas. Todavia, elas são ineficientes quando falamos em sistemas distribuídos, formados por várias CPUs interligadas por uma rede. Para resolver essa limitação, surge a estratégia de troca de mensagens (message passing).
- Enquanto uma mensagem não chega, o receptor permanece bloqueado ou emite um código de erro. Para evitar mensagens perdidas, ao receber uma mensagem, o receptor pode enviar uma mensagem de confirmação de recebimento (acknowledgement).

### Seção 4
- Os programas não concorrentes são mais fáceis de serem escritos, analisados e modificados do que os programas concorrentes. Porém, a necessidade de resolver problemas por meio do paralelismo, somada ao surgimento de sistemas multiprocessados, distribuídos e as arquiteturas computacionais paralelas, aumentou o uso de programas concorrentes.
- O monitor é um objeto que contém dados e procedimentos necessários para realizar a alocação de determinado recurso compartilhado ou um grupo de recursos compartilhados reutilizáveis serialmente.
- 