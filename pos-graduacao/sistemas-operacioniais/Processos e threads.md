### Seção 1
- Um processo possui o seu próprio espaço de endereçamento, composto de três regiões: uma região de texto, onde é armazenado o código a ser executado pelo processador; uma região de dados, onde são armazenadas as variáveis e a memória, que é alocada dinamicamente durante a execução do processo; e uma região de pilha, onde são armazenadas as instruções e as variáveis locais para as chamadas ativas do procedimento, que fazem a pilha crescer quando são emitidas e diminuem quando o procedimento retorna às chamadas.
- Uma das responsabilidades do sistema operacional é garantir que cada processo tenha acesso à mesma quantidade de tempo de uso de CPU. Entretanto, o número superior de processos, comparado ao número de CPUs disponíveis, somado à concorrência na execução dos processos, faz com que o trabalho do sistema operacional seja mais difícil.
- Um processo usa diferentes estados, como: o estado de execução, que indica que um processo está sendo executado pelo processador; o estado “de pronto”, no qual o processo informa ao sistema operacional que já pode ser executado e está aguardando por um processador que esteja disponível; e o estado bloqueado, quando o processo está aguardando pela finalização de um evento, por exemplo, uma requisição de E/S para prosseguir com sua execução.
- Cabe ao sistema operacional conhecer os processos e seus respectivos estados. Para realizar essa tarefa, ele utiliza duas listas, uma que armazena os processos em estado de pronto, lista de prontos; e outra que armazena os processos em estado bloqueado, lista de bloqueados.
- Os processos da lista de prontos são armazenados de acordo com a sua prioridade, da mais alta para a mais baixa.
- Os sistemas operacionais são responsáveis pelos processos, prestando serviços essenciais a eles, como criação, destruição, suspensão, retomada, mudança de prioridade, bloqueio, ativação e a comunicação interprocessos (Interprocess Communication – IPC).
- A lista de prontos recebe processos de acordo com o que um usuário executa de programas em seu computador.
- Os processos vão galgando posições na lista de prontos, deixando o estado de pronto para o estado de execução, sofrendo uma transição de estado.
- Cabe a uma entidade do sistema operacional, chamada despachante, a responsabilidade de indicar ao processo, que ocupa a primeira posição na lista de prontos, que ele
pode fazer uso de um processador, recebendo essa atividade o nome de despacho.
- O sistema operacional utiliza um timer, isto é, um temporizador de intervalo que permite que um processo seja executado durante um intervalo de tempo específico, chamado quantum 1 . Caso o processo não libere o processador, após o seu limite de tempo de execução expirar, o timer cria uma interrupção, permitindo que o sistema operacional recupere
o controle sobre o processador.
- O processo terá o seu estado alterado de execução para pronto, pelo sistema operacional, que também mudará o estado do primeiro processo da fila de pronto para execução,
iniciando de novo o timer.
- 1) Quando um processo é despachado, o sistema operacional muda seu estado de pronto para execução.
- 2) Quando um evento que é solicitado por um processo que está no estado bloqueado se encerra, ele muda de bloqueado para pronto.
- 3) Quando o quantum de execução de um processo se encerra e ele conseguiu realizar a(s) tarefa(s) desejada(s), ele muda de execução para pronto.
- 4) Quando o quantum de execução de um processo se encerra e ele não conseguiu realizar a(s) tarefa(s) desejada(s), ele muda de execução para bloqueado. Essa é a única transição de estado que é feita pelo próprio processo.
- Quando um processo é criado pelo sistema operacional, ele recebe um número para a sua identificação, o Número de Identificação de Processo (Process Identification Number – PIN).
- Ainda, recebe um conjunto de informações, denominado Bloco de Controle de Processo ou descritor de processo (Process Control Block – PCB), que auxiliará o sistema operacional a gerenciá-lo.
- A tabela de processos permite que o sistema operacional acesse mais rapidamente os PCBs. Quando um processo é encerrado, voluntariamente ou não, o sistema operacional retira o registro desse processo da tabela de processos, disponibilizando os recursos que ele estava utilizando e liberando-os para outros.
- Para detectar possíveis ameaças à segurança, ou para depuração (debug) do software em execução, os sistemas operacionais permitem aos administradores, usuários e processos realizarem a suspensão de um processo. Quando isso ocorre, ele não é destruído, mas retirado da disputa pelo tempo de execução na CPU sem previsão de retorno.
- Quando um processo está em execução e suspende a si mesmo, ele passa para o estado suspenso-pronto, que posteriormente será alterado para pronto. Quando a suspensão é feita por um outro processo, temos duas opções: se o processo suspenso estiver no estado pronto, o seu estado mudará para suspenso-pronto; porém, se ele estiver no estado bloqueado, o seu estado mudará para suspenso-bloqueado.
- Dizemos que um sistema operacional realiza um chaveamento de contexto quando ele interrompe a execução de um processo e começa a executar outro que estava no estado pronto. Quando isso ocorre, o sistema operacional salva o contexto de execução do processo que estava no estado execução no PCB desse processo.
- As interrupções habilitam o software a responder aos sinais do hardware. Para poder gerenciar melhor as interrupções, o sistema operacional usa um conjunto de instruções, chamado tratador de interrupção, no qual ele armazena a resposta para cada tipo de interrupção que pode ocorrer, controlando melhor o processador.
- Os ambientes de multiprogramação e os ambientes de rede obrigaram os sistemas operacionais a criarem formas de comunicação interprocessos (IPC) que garantem a coordenação (sincronização) das atividades dos processos fazendo uso de sinais e trocas de mensagem para eles se comunicarem entre si.
- Sinais são interrupções de software que notificam ao processo que um evento ocorreu. Eles não permitem que os processos troquem dados entre si. Fica sob responsabilidade do sistema operacional determinar qual processo deverá receber o sinal e como ele será respondido pelo processo que o recebeu.
- Ao receber um sinal, os processos podem: capturá-lo, quando o processo especifica uma rotina utilizada pelo sistema operacional para emitir o sinal; ignorá-lo, quando o processo depende de uma ação-padrão realizada pelo sistema operacional para que o sinal recebido possa ser tratado; ou mascará-lo, quando o processo informa ao sistema
operacional que não deseja mais receber sinais daquele tipo até que a máscara do sinal seja bloqueada.
- Com o aumento do interesse pelo uso de sistemas distribuídos, a IPC passou a ser realizada por trocas de mensagens, que podem ser 88 Sistemas Operacionais
unidirecionais, isto é, quando um processo atua como emissor e outro como receptor da mensagem; ou bidirecional, em que cada um dos processos pode atuar como emissor ou receptor. As mensagens podem ter envios bloqueantes (comunicação síncrona) ou envios não bloqueantes (comunicação assíncrona).

### Seção 2
- Um thread permite ao sistema operacional executar uma tarefa de modo independente dos outros processos ou threads, sendo chamado de processo leve (Lightweight Process – LWP). Apesar dessa independência, os threads são criados com base em processo tradicional, chamado de processo pesado (Heavyweight Process – HWP). Threads usam um subconjunto dos recursos utilizados por um processo comum, como: os registradores, a pilha e os Dados Específicos de Threads (Thread-Specific Data – TSD). O espaço de endereço e outras informações globais são compartilhadas pelos threads com o processo pesado.
- O padrão POSIX é uma especificação que busca definir padrões para que os sistemas operacionais sejam compatíveis entre si. Ele define uma API, os shells de linha de comando e as interfaces dos aplicativos para o UNIX e os sistemas com base nele.
- Quando criamos um thread com a linguagem de programação Java, ele está no estado nascido (born), permanecendo assim até que o programa o inicie e passe para o estado pronto
(runnable – executável). Quando ele tem acesso a um processador e começa a ser executado, muda para o estado em execução. Finalmente, indo para o estado morto (dead), que é quando termina sua tarefa ou é encerrado, liberando seus recursos no sistema.
- Apesar de a forma de se implementar threads variar entre os sistemas operacionais, na sua grande maioria, eles utilizam os três modelos mais conhecidos: threads de usuário, threads de núcleo e threads de usuário e de núcleo.
- O sistema operacional visualiza todos os threads que compõem um processo multithread, como apenas um único bloco de execução, que é despachado de uma só vez, e não por thread. Isso recebe o nome de mapeamento de thread muitos-para-um.
- Como desvantagens no seu uso, destacamos: o núcleo (core) do sistema operacional considera um processo multithread como sendo um único bloco de execução, isso impede que o sistema operacional despache threads para serem executados em vários processadores simultaneamente; outro ponto negativo é que, caso um dos threads realize uma requisição para um dispositivo de E/S, todo o processo ficará bloqueado até que a requisição seja encerrada.
- A combinação de threads de usuário e threads de núcleo usa o mapeamento de threads muitos-para-muitos (mapeamento de threads m-to-n), no qual o número de threads de usuário e threads de núcleo não precisa ser igual. Em comparação com o mapeamento de threads um-para-um, o mapeamento de threads muitos-para-muitos consegue reduzir a sobrecarga do sistema operacional, implementando um reservatório de threads (thread pooling), em que a aplicação informa ao sistema operacional o número de threads de núcleo que precisa.

### Seção 3
- Se houver mais de um thread em execução no sistema operacional, dizemos que esses threads são concorrentes entre si. Eles podem estar em execução de maneira independente ou cooperativa. Quando a execução é independente, mas os threads eventualmente se comunicam, chamamos esse evento de execução assíncrona.
 - Enquanto um thread está manipulando a variável, os demais precisam esperar (exclusão mútua). Cabe ao sistema operacional organizar esse processo, limitando o acesso à variável para apenas um dos threads e enfileirando os demais, que aguardam a sua vez. Esse processo é chamado de serialização.
 - Quando um thread acessa uma área que possui dados que podem ser modificados, chamada de seção ou região crítica, ele deve observar se o thread deseja realizar uma operação de leitura que não vai, por exemplo, afetar o conteúdo armazenado. Nesse caso, podemos encontrar threads concorrentes acessando a mesma região crítica.
 - Quatro condições precisam ser atendidas para que tenhamos uma boa solução de gerenciamento ao acesso da região crítica:<br>
a. Dois processos não podem estar acessando simultaneamente as suas regiões críticas.<br>
b. Não há certeza no que se refere à velocidade e ao número de CPUs disponíveis.<br>
c. Um processo que está em execução, fora da sua região crítica, não pode bloquear outros processos.<br>
d. Um processo não pode ficar esperando infinitamente para acessar a região crítica.
- Quando pensamos em uma CPU, mono ou multiprocessada, as estratégias citadas anteriormente são válidas. Todavia, elas são ineficientes quando falamos em sistemas distribuídos, formados por várias CPUs interligadas por uma rede. Para resolver essa limitação, surge a estratégia de troca de mensagens (message passing).
- Enquanto uma mensagem não chega, o receptor permanece bloqueado ou emite um código de erro. Para evitar mensagens perdidas, ao receber uma mensagem, o receptor pode enviar uma mensagem de confirmação de recebimento (acknowledgement).

### Seção 4
- Os programas não concorrentes são mais fáceis de serem escritos, analisados e modificados do que os programas concorrentes. Porém, a necessidade de resolver problemas por meio do paralelismo, somada ao surgimento de sistemas multiprocessados, distribuídos e as arquiteturas computacionais paralelas, aumentou o uso de programas concorrentes.
- O monitor é um objeto que contém dados e procedimentos necessários para realizar a alocação de determinado recurso compartilhado ou um grupo de recursos compartilhados reutilizáveis serialmente.
- Os dados do monitor somente são acessados por threads que estão dentro dele, sem a possibilidade de acesso por threads que estão do lado de fora. Essa técnica é chamada de ocultação de informações.
- Se um thread chama a rotina de entrada do monitor e não há nenhum thread sendo executado dentro dele, ele terá acesso ao monitor. Caso contrário, ele terá de esperar até que o monitor seja destravado. Quando isso ocorre, o monitor cria uma fila de espera, priorizando os que estão na fila há mais tempo.
- Quando um thread, que está dentro do monitor, descobre que não pode continuar, ele executa um wait sobre uma variável condicional, informando, por exemplo, que o buffer que ele estava utilizando está cheio. Ao fazer isso, o thread fica bloqueado, permitindo que outro thread que está na fila tenha acesso ao monitor. Quando o buffer estiver vazio novamente, o thread que está em execução emite um sinal, sai do monitor e permite que o outro thread, até então bloqueado, volte a ocupar o monitor.
- Quando um thread produtor entrega dados para um thread consumidor, por meio de um buffer em comum, dizemos que eles usam um buffer circular ou limitado (Figura 13), pois o buffer vai sendo preenchido pelo thread produtor, eventualmente mais rápido que o thread consumidor, que vai retirando os dados do buffer gradualmente. Quando o buffer está cheio, o thread produtor é obrigado a “dar a volta” no buffer e começar a preenchê-lo novamente desde a primeira posição. Como os threads podem efetuar suas tarefas com velocidades diferentes, faz-se necessária uma sincronização eficiente para que os dois consigam desempenhar suas atividades da melhor maneira possível.
- O buffer circular permite ao thread produtor escrever dados no buffer, sem ter de esperar que o thread consumidor o retire em seguida, fazendo uso das entradas vazias do buffer circular. Isso não impede que o thread consumidor faça a leitura dos dados na ordem correta e melhore o desempenho do sistema, pois o thread produtor pode escrever os dados continuamente.

### Seção 5
- Um processo ou thread está em estado de deadlock (impasse) ou travado se estiver esperando por um evento que não vai acontecer.
- Um impasse pode ocorrer quando processos disputam recursos, que podem ser: um dispositivo de hardware, como uma unidade de disco rígido, ou trecho de informação, como um  registro armazenado em um banco de dados.
- Um recurso é algo que pode ser adquirido, usado e liberado com o passar do tempo. Como podemos observar, há dois tipos de recursos:<br>
a. Preemptível: pode ser retirado do seu processo proprietário sem prejudicá-lo, como o uso da memória no computador.<br>
b. Não preemptível: não pode ser retirado do seu processo proprietário sem causar prejuízo à atividade que estava sendo realizada – por exemplo, quando acaba a luz durante a gravação de um CD-ROM, comprometendo esta.
- Os impasses ocorrem com recursos não preemptíveis, visto que os recursos preemptíveis conseguem alocar recursos de um processo para o outro.
- Quando um processo necessita fazer uso de um recurso que não está disponível naquele momento, ele é bloqueado pelo sistema operacional e, quando o recurso estiver disponível, será acordado pelo sistema operacional. Em outros casos, ao se deparar com um recurso indisponível, o processo emitirá um código de erro.
- Em determinados casos, o uso de recursos é gerenciado por processos de usuário, como quando o usuário tenta acessar os registros armazenados em um banco de dados. Uma forma de conceder aos processos de usuários o gerenciamento dos recursos é fazer uso de semáforos.
- Um conjunto de processos entrará em uma situação de impasse quando todos os processos desse conjunto estiverem esperando por um evento que somente outro processo do mesmo conjunto pode realizar. Esse tipo de impasse é denominado impasse de recurso.
- Existem quatro condições que podem gerar um impasse de recurso:<br>
a. Condição de exclusão mútua – quando um recurso está associado a um único processo ou disponível.<br>
b. Condição de posse e espera – em que os processos já estão fazendo uso de recursos e precisam fazer uso de outros recursos.<br>
c. Condição de não preempção – na qual os recursos que já estão sendo utilizados por um processo não podem ser tomados dele, tendo de ser liberados por eles voluntariamente.<br>
d. Condição de espera circular – em que ocorre um encadeamento circular de processos, que estão esperando pela liberação de um recurso que está sendo utilizado por outro processo, pertencente à mesma cadeia.
- Um método utilizado pelos projetistas de sistemas operacionais para a recuperação de um processo que entrou em um impasse é a geração de pontos de salvaguardo (checkpoints), que permitem ao processo armazenar o seu estado (uso de memória e recursos alocados) em um arquivo, podendo ser reinicializados com base nessa informação.
- Outra abordagem é finalizar um dos processos envolvidos, esperando que o seu fim permita que os demais processos realizem suas tarefas, sendo chamada de recuperação por meio de eliminação de processos.
- Uma possível estratégia para evitar a ocorrência de impasses são as trajetórias de recursos, que têm como base o conceito de estados seguros, no qual um estado é assim considerado quando ele não está envolvido em um impasse e caso haja uma ordem de escalonamento (decisão de quem vai utilizar o recurso ou esperá-lo) para que ele possa concluir sua execução. Já no estado inseguro, o sistema operacional não pode garantir que o processo será concluído.
- Outra estratégia é fazer uso do algoritmo do banqueiro, desenvolvido por Dijkstra em 1965, em que um banqueiro (sistema operacional) de uma cidade pequena consegue atender um grupo de clientes (processos ou threads) que necessitam de uma linha de crédito (recursos). Cabe ao banqueiro verificar se o pedido pode levar a um estado inseguro, o que leva à aprovação ou à recusa do pedido.
- Considerando que as soluções propostas para evitar a ocorrência de impasses não podem ser aplicadas de maneira genérica, algumas aplicações passaram a fazer uso de algoritmos para resolver problemas específicos, como:<br>
a. Bloqueio em duas fases (two-phase blocking): atende à necessidade dos sistemas gerenciadores de bancos de dados que realizam o bloqueio (lock) de alguns registros para que eles possam ser atualizados.<br>
b. Impasse de comunicação: pode ocorrer quando um processo X envia uma mensagem para o processo Y e fica bloqueado até que a mensagem de retorno seja recebida.<br>
c. Livelock: pode acontecer quando a tabela de processos do sistema operacional está cheia e um processo que tenta criar threads descobre isso. Ele não consegue criar os threads naquele momento, sendo obrigado a esperar um pouco e fazer uma nova tentativa. O UNIX e o Windows ignoram o problema de livelock, uma vez que consideram que sua ocorrência é menos traumática do que limitar a experiência do usuário.<br>
d. Condição de inanição (starvation): pode acontecer quando um processo nunca tenha acesso a um recurso que está solicitando.


### Seção 6
- Quando um computador possui vários processos ou threads no estado de pronto e apenas um processador livre para atendê-los, o sistema operacional precisa usar um escalonador, que usa o algoritmo de escalonamento para chegar a uma decisão.
- Os algoritmos de escalonamento podem ser divididos em duas categorias com relação às interrupções de E/S: na primeira, o algoritmo de escalonamento não preemptivo escolhe um processo para execução e o executa até que ele seja bloqueado, para uma requisição de E/S, por outro processo, ou quando o processo libera a CPU por conta própria; na segunda, o algoritmo de escalonamento preemptivo escolhe um processo para execução e o executa por um tempo máximo definido.
- Há três categorias de algoritmos de escalonamento: os sistemas em lote, os sistemas interativos e os sistemas com restrição de tempo real.<br>
Os sistemas em lote são utilizados em empresas que operam com atividades realizadas periodicamente, como folhas de pagamento, contas a pagar etc. Neles, não há terminais de usuários aguardando pela resposta da execução dos processos, logo, os algoritmos de escalonamento preemptivos e não preemptivos são aceitáveis.
- Nos sistemas interativos, em que há usuários interagindo e aguardando uma resposta, faz-se necessário o uso do algoritmo de escalonamento preemptivo para evitar que um processo tome o controle da CPU. Os servidores também se enquadram nessa categoria.
- Nos sistemas com restrição de tempo real, em que os processos sabem que não podem ser executados por um longo período, o algoritmo de escalonamento preemptivo não faz sentido. Esse tipo de sistema executa somente processos relacionados ao seu propósito. Esses sistemas consideram as seguintes características como essenciais para o algoritmo de escalonamento: cumprimento de prazos, evitando que os dados sejam perdidos durante a execução; e previsibilidade, evitando perda de qualidade durante a execução do processo.
